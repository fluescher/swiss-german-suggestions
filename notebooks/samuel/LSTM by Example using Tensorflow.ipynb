{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM by Example using Tensorflow\n",
    "https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return LSTMConfig\n",
    "\n",
    "class LSTMConfig:\n",
    "    # Input / output\n",
    "    training_file = 'LSTM_by_Example_data/von_tavel.txt'\n",
    "    output_dir = 'LSTM_by_Example_output'\n",
    "    model_file = output_dir + '/LSTM_by_Example_model'\n",
    "    \n",
    "    # Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 100000\n",
    "    display_step = 1000\n",
    "    n_input = 3\n",
    "\n",
    "    # number of units in RNN cell\n",
    "    n_hidden = 512\n",
    "    \n",
    "    # Use two layer RNN cells\n",
    "    two_layer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        content = [line.rstrip() for line in f]\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = [word for line in content for word in line ] # flatten list\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(config, x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, config.n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x, config.n_input, 1)\n",
    "\n",
    "    if config.two_layer:\n",
    "        # 2-layer LSTM, each layer has n_hidden units.\n",
    "        # Average Accuracy= 95.20% at 50k iter\n",
    "        rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(config.n_hidden), rnn.BasicLSTMCell(config.n_hidden)])\n",
    "    else:\n",
    "        # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "        # Average Accuracy= 90.60% 50k iter\n",
    "        rnn_cell = rnn.BasicLSTMCell(config.n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.add(tf.matmul(outputs[-1], weights['out']), biases['out'], name='rnn_predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "\n",
    "    def __init__(self, config, vocab_size):\n",
    "        # tf Graph input\n",
    "        x = tf.placeholder(\"float\", [None, config.n_input, 1], name='x')\n",
    "        y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "        # RNN output node weights and biases\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([config.n_hidden, vocab_size]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "        }\n",
    "    \n",
    "        pred = RNN(config, x, weights, biases)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # TODO: cleanup\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.pred = pred\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session, model, config, training_data):\n",
    "    step = 0\n",
    "    offset = random.randint(0, config.n_input+1)\n",
    "    end_offset = config.n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    # save meta model and prepare saver for state\n",
    "    tf.train.export_meta_graph(filename=config.model_file+'.meta')\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while step < config.training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, config.n_input+1)\n",
    "\n",
    "        # [ offset, offset+1, offset+2] -> [ word1, word2, word3 ] -> [ index1, index2, index3 ]\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+config.n_input) ]\n",
    "        # [ index1, index2, index3 ] -> array([[[1],[2],[3]]])\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+config.n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([model.optimizer, model.accuracy, model.cost, model.pred], \\\n",
    "                                                feed_dict={model.x: symbols_in_keys, model.y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % config.display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/config.display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/config.display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + config.n_input)]\n",
    "            symbols_out = training_data[offset + config.n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "            # save the current state\n",
    "            saver.save(session, config.model_file,global_step=step+1, write_meta_graph=False)\n",
    "        step += 1\n",
    "        offset += (config.n_input + 1)\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 10.972920, Average Accuracy= 2.90%\n",
      "['Grund', 'gnoh,', 'wäre'] - [di] vs [het]\n",
      "Iter= 2000, Average Loss= 9.801846, Average Accuracy= 2.80%\n",
      "['gschpürt,', 'daß', 'es'] - [dem] vs [es]\n",
      "Iter= 3000, Average Loss= 9.886020, Average Accuracy= 2.30%\n",
      "['duvet', 'und', 'amene'] - [Bärg] vs [nid]\n",
      "Iter= 4000, Average Loss= 9.573025, Average Accuracy= 2.80%\n",
      "['Ärde', \"z'scheide.\", \"D'Predig\"] - [het] vs [und]\n",
      "Iter= 5000, Average Loss= 9.622506, Average Accuracy= 3.00%\n",
      "['heig', 'müesse', 'zahle.'] - [Und] vs [het]\n",
      "Iter= 6000, Average Loss= 9.702545, Average Accuracy= 3.10%\n",
      "['er', 'tha', 'het.'] - [Aber] vs [der]\n",
      "Iter= 7000, Average Loss= 9.747376, Average Accuracy= 3.70%\n",
      "['geng', \"d'Uhge\", 'zue,'] - [wenn] vs [und]\n",
      "Iter= 8000, Average Loss= 9.670922, Average Accuracy= 2.50%\n",
      "[\"d'Ouge\", 'hei-n-ihm', 'uheimelig'] - [gluuchtet.] vs [und]\n",
      "Iter= 9000, Average Loss= 9.517886, Average Accuracy= 4.00%\n",
      "['über', \"d'Rotheche-n-und\", 'um'] - [jede] vs [het]\n",
      "Iter= 10000, Average Loss= 9.617063, Average Accuracy= 3.10%\n",
      "['di', 'bessere-n-Ohre', 'gha'] - [het,] vs [und]\n",
      "Iter= 11000, Average Loss= 9.657250, Average Accuracy= 2.30%\n",
      "['12', 'Lombach', 'het'] - [der] vs [der]\n",
      "Iter= 12000, Average Loss= 9.601132, Average Accuracy= 4.90%\n",
      "[\"d'Frou\", 'Landorfer', 'über'] - [ihn] vs [het]\n",
      "Iter= 13000, Average Loss= 9.798366, Average Accuracy= 2.70%\n",
      "['merci', 'de', 'vos'] - [compliments.] vs [Herr]\n",
      "Iter= 14000, Average Loss= 9.568599, Average Accuracy= 2.20%\n",
      "[\"z'säge,\", 'daß', 'es'] - [höchscht] vs [es]\n",
      "Iter= 15000, Average Loss= 9.627139, Average Accuracy= 3.30%\n",
      "['»Hinder', 'däm', 'Meitschi'] - [schteckt] vs [het]\n",
      "Iter= 16000, Average Loss= 9.663677, Average Accuracy= 2.80%\n",
      "['Landorfer,', 'daß', 'der'] - [»Bayard«,] vs [der]\n",
      "Iter= 17000, Average Loss= 9.540655, Average Accuracy= 3.30%\n",
      "['–', '–', '–'] - [Und] vs [und]\n",
      "Iter= 18000, Average Loss= 9.618391, Average Accuracy= 3.60%\n",
      "['gsi', 'und', 'wär'] - [ohni] vs [und]\n",
      "Iter= 19000, Average Loss= 9.410836, Average Accuracy= 4.30%\n",
      "['das', 'Genre', 'vo'] - [Antworte] vs [het]\n",
      "Iter= 20000, Average Loss= 9.440917, Average Accuracy= 3.80%\n",
      "['verschideni', 'chlyneri', 'Schtriche-n-ufglöst,'] - [i] vs [het]\n",
      "Iter= 21000, Average Loss= 9.815333, Average Accuracy= 3.40%\n",
      "['mit', 'dem', 'Jetti'] - [ga] vs [der]\n",
      "Iter= 22000, Average Loss= 9.478898, Average Accuracy= 2.70%\n",
      "['Dobe', 'hei', 'sech'] - [ändlech] vs [der]\n",
      "Iter= 23000, Average Loss= 9.488665, Average Accuracy= 3.70%\n",
      "['gäge', 'Rubige', 'zue'] - [marschiert] vs [und]\n",
      "Iter= 24000, Average Loss= 9.341003, Average Accuracy= 3.60%\n",
      "['seit:', '»Ja,', 'die!'] - [–] vs [und]\n",
      "Iter= 25000, Average Loss= 9.543860, Average Accuracy= 3.60%\n",
      "['sy', 'liebe', 'Chaumont'] - [gange.] vs [und]\n",
      "Iter= 26000, Average Loss= 9.235489, Average Accuracy= 4.20%\n",
      "['Ha-n-i', 'nid', 'e'] - [Familie,] vs [der]\n",
      "Iter= 27000, Average Loss= 9.320880, Average Accuracy= 4.10%\n",
      "['bhalte', 'hei', 'vo'] - [der] vs [der]\n",
      "Iter= 28000, Average Loss= 9.491119, Average Accuracy= 3.30%\n",
      "['si', 'hei', 'ga'] - [erzelle,] vs [der]\n",
      "Iter= 29000, Average Loss= 9.356990, Average Accuracy= 3.50%\n",
      "['beide', 'junge', 'Manne-n-es'] - [paar] vs [het]\n",
      "Iter= 30000, Average Loss= 9.446269, Average Accuracy= 3.90%\n",
      "['Houpme', 'Lombach', 'uf'] - [d'Syte] vs [der]\n",
      "Iter= 31000, Average Loss= 9.542588, Average Accuracy= 2.20%\n",
      "['nid', 'öppe', \"z'Burdlef\"] - [gsi,] vs [der]\n",
      "Iter= 32000, Average Loss= 9.370472, Average Accuracy= 4.00%\n",
      "['Lombach', 'vore', 'Wald'] - [use] vs [der]\n",
      "Iter= 33000, Average Loss= 9.449973, Average Accuracy= 3.00%\n",
      "['hi', 'ga', 'Rychebach'] - [ds] vs [der]\n",
      "Iter= 34000, Average Loss= 9.568452, Average Accuracy= 3.50%\n",
      "['mache,', 'ohni', 'rächti'] - [Befähli] vs [der]\n",
      "Iter= 35000, Average Loss= 9.548255, Average Accuracy= 2.90%\n",
      "['gsi,', 'als', 'bi'] - [allne-n-andere] vs [der]\n",
      "Iter= 36000, Average Loss= 9.251625, Average Accuracy= 4.50%\n",
      "['Einsamkeit', 'zwüsche', 'de'] - [Tanne,] vs [der]\n",
      "Iter= 37000, Average Loss= 9.382927, Average Accuracy= 3.20%\n",
      "['1806', 'wieder', 'mit'] - [sym] vs [der]\n",
      "Iter= 38000, Average Loss= 9.256443, Average Accuracy= 3.10%\n",
      "['Félicie', 'füre', 'zoge.'] - [Zerscht] vs [het]\n",
      "Iter= 39000, Average Loss= 9.744144, Average Accuracy= 2.80%\n",
      "[\"het's\", 'uf', 'ds'] - [Vieh] vs [der]\n",
      "Iter= 40000, Average Loss= 9.450010, Average Accuracy= 4.20%\n",
      "['»Häb', 'nid', 'Angscht,«'] - [seit] vs [und]\n",
      "Iter= 41000, Average Loss= 9.404619, Average Accuracy= 3.60%\n",
      "['ganz', 'allei', 'und'] - [so] vs [het]\n",
      "Iter= 42000, Average Loss= 9.342215, Average Accuracy= 3.90%\n",
      "['winkt', 'ihm,', 'redt'] - [ne] vs [und]\n",
      "Iter= 43000, Average Loss= 9.323670, Average Accuracy= 3.50%\n",
      "['Soum', 'vom', 'vordere'] - [Bosquet] vs [het]\n",
      "Iter= 44000, Average Loss= 9.494246, Average Accuracy= 3.00%\n",
      "['wie', 'wenns', 'de'] - [Zigüüner] vs [und]\n",
      "Iter= 45000, Average Loss= 9.446765, Average Accuracy= 2.70%\n",
      "['Scho', 'z’', 'morndrisch'] - [het] vs [und]\n",
      "Iter= 46000, Average Loss= 9.478889, Average Accuracy= 4.00%\n",
      "['het', 'es', 'syni'] - [Tälpli] vs [het]\n",
      "Iter= 47000, Average Loss= 9.482467, Average Accuracy= 3.90%\n",
      "['das', 'heig', 'der'] - [Xandi] vs [isch]\n",
      "Iter= 48000, Average Loss= 9.367740, Average Accuracy= 3.20%\n",
      "['ungstört', 'blibe.', 'Das'] - [Chöli,] vs [het]\n",
      "Iter= 49000, Average Loss= 9.439319, Average Accuracy= 3.20%\n",
      "['länge', 'Finger', 'öppis'] - [usem] vs [und]\n",
      "Iter= 50000, Average Loss= 9.256778, Average Accuracy= 3.20%\n",
      "['unglychlige', 'Paar', 'verabschidet.'] - [Dem] vs [isch]\n",
      "Iter= 51000, Average Loss= 9.199584, Average Accuracy= 3.90%\n",
      "['gseit', 'gsi,', 'daß'] - [wyters] vs [der]\n",
      "Iter= 52000, Average Loss= 9.317036, Average Accuracy= 4.00%\n",
      "['weiß', 'es', 'schynts'] - [überall.] vs [het]\n",
      "Iter= 53000, Average Loss= 9.471447, Average Accuracy= 3.50%\n",
      "['me', 'se', 'söll'] - [alleini] vs [der]\n",
      "Iter= 54000, Average Loss= 9.365379, Average Accuracy= 4.00%\n",
      "['het', 'me', 'du'] - [üsne] vs [der]\n",
      "Iter= 55000, Average Loss= 9.353886, Average Accuracy= 3.80%\n",
      "['Si', 'sy', 'scho'] - [tief] vs [der]\n",
      "Iter= 56000, Average Loss= 9.337670, Average Accuracy= 3.40%\n",
      "['Haselmuus.', 'Er', 'geit'] - [i] vs [und]\n",
      "Iter= 57000, Average Loss= 9.353383, Average Accuracy= 3.20%\n",
      "['nid', 'gmacht.', 'Düregschliche'] - [ha-n-i] vs [und]\n",
      "Iter= 58000, Average Loss= 9.417917, Average Accuracy= 3.80%\n",
      "['und', 'kei', 'Chatz'] - [meh] vs [isch]\n",
      "Iter= 59000, Average Loss= 9.580629, Average Accuracy= 3.60%\n",
      "['im', 'Wasser,', 'drücke'] - [si] vs [und]\n",
      "Iter= 60000, Average Loss= 9.440124, Average Accuracy= 2.70%\n",
      "['afa', 'trummle,', 'und'] - [wo] vs [und]\n",
      "Iter= 61000, Average Loss= 9.371178, Average Accuracy= 4.20%\n",
      "['chönnen', 'i', 'd’Stube'] - [trage.] vs [und]\n",
      "Iter= 62000, Average Loss= 9.520272, Average Accuracy= 2.60%\n",
      "['Merze', 'gfangen', 'und'] - [i] vs [der]\n",
      "Iter= 63000, Average Loss= 9.488518, Average Accuracy= 2.90%\n",
      "['abtreit,', 'vowäge', 'jitz'] - [isch] vs [und]\n",
      "Iter= 64000, Average Loss= 9.666547, Average Accuracy= 3.00%\n",
      "['Sergeant', 'isch', 'es'] - [gsi,] vs [und]\n",
      "Iter= 65000, Average Loss= 9.484932, Average Accuracy= 3.70%\n",
      "['lang', 'gange,', 'so'] - [ghört] vs [der]\n",
      "Iter= 66000, Average Loss= 9.392017, Average Accuracy= 4.30%\n",
      "['male', 'tuen', 'i'] - [dert] vs [der]\n",
      "Iter= 67000, Average Loss= 9.778886, Average Accuracy= 3.50%\n",
      "['ha', 'dä', 'no'] - [vom] vs [der]\n",
      "Iter= 68000, Average Loss= 9.531363, Average Accuracy= 2.40%\n",
      "['in', 'Ängiland.', 'Aber'] - [vom] vs [der]\n",
      "Iter= 69000, Average Loss= 9.503317, Average Accuracy= 3.50%\n",
      "['wenn', 'e', 'guet'] - [ustüfteleti] vs [und]\n",
      "Iter= 70000, Average Loss= 9.449489, Average Accuracy= 2.90%\n",
      "['gfalle.', 'Ihri', 'ganzi'] - [Alegi] vs [und]\n",
      "Iter= 71000, Average Loss= 9.562344, Average Accuracy= 2.70%\n",
      "['Gang', 'lig', 'du'] - [jitz] vs [und]\n",
      "Iter= 72000, Average Loss= 9.585044, Average Accuracy= 3.90%\n",
      "['herrjeh!', 'Geng', 'no'] - [nüt] vs [und]\n",
      "Iter= 73000, Average Loss= 9.631477, Average Accuracy= 2.90%\n",
      "['Er', 'versorget', 'sys'] - [Glas] vs [und]\n",
      "Iter= 74000, Average Loss= 9.528116, Average Accuracy= 2.40%\n",
      "['sälber.', 'Der', 'Bscheid'] - [söll] vs [und]\n",
      "Iter= 75000, Average Loss= 9.443314, Average Accuracy= 3.20%\n",
      "['und', 'hinder', 'den'] - [Ouge] vs [und]\n",
      "Iter= 76000, Average Loss= 9.529275, Average Accuracy= 1.90%\n",
      "['—', '—', 'Es'] - [sy] vs [und]\n",
      "Iter= 77000, Average Loss= 9.465913, Average Accuracy= 2.30%\n",
      "['Jitz', 'isch', 'du'] - [undereinisch] vs [und]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 78000, Average Loss= 9.200858, Average Accuracy= 3.80%\n",
      "['öppis', 'z’säge,', 'fat'] - [der] vs [und]\n",
      "Iter= 79000, Average Loss= 9.417497, Average Accuracy= 2.90%\n",
      "['der', 'Nasen', 'i'] - [ds] vs [und]\n",
      "Iter= 80000, Average Loss= 9.442804, Average Accuracy= 3.00%\n",
      "['vo', 'dene', 'Mannen'] - [isch] vs [und]\n",
      "Iter= 81000, Average Loss= 9.476198, Average Accuracy= 4.00%\n",
      "['üsi', 'heimeligi,', 'dütlechi,'] - [alti] vs [und]\n",
      "Iter= 82000, Average Loss= 9.382096, Average Accuracy= 3.90%\n",
      "['am', 'Morge', 'vom'] - [Yrückungstag] vs [und]\n",
      "Iter= 83000, Average Loss= 9.609082, Average Accuracy= 1.90%\n",
      "['dem', 'Brügiwage.', 'Me'] - [büret] vs [und]\n",
      "Iter= 84000, Average Loss= 9.668354, Average Accuracy= 2.60%\n",
      "['hei,', 'wi', 'ne'] - [d’Längizyti] vs [und]\n",
      "Iter= 85000, Average Loss= 9.370621, Average Accuracy= 3.30%\n",
      "['scho', 'alles', 'im'] - [reine] vs [und]\n",
      "Iter= 86000, Average Loss= 9.480215, Average Accuracy= 4.70%\n",
      "['und', 'Lache,', 'wo'] - [di] vs [der]\n",
      "Iter= 87000, Average Loss= 9.404656, Average Accuracy= 3.20%\n",
      "['heig', 'de', 'de'] - [Gloggen] vs [der]\n",
      "Iter= 88000, Average Loss= 9.531353, Average Accuracy= 3.40%\n",
      "['gno', 'hei.»', '«Das'] - [weiß] vs [und]\n",
      "Iter= 89000, Average Loss= 9.274357, Average Accuracy= 3.50%\n",
      "['heißt,', 'mer', 'müeße'] - [vergä!»] vs [het]\n",
      "Iter= 90000, Average Loss= 9.622017, Average Accuracy= 2.50%\n",
      "['well', 'di', 'Sach'] - [gar] vs [und]\n",
      "Iter= 91000, Average Loss= 9.538890, Average Accuracy= 2.20%\n",
      "['dermit,', 'so', 'näme'] - [si] vs [i]\n",
      "Iter= 92000, Average Loss= 9.406402, Average Accuracy= 4.20%\n",
      "['brichte,', 'er', 'heig'] - [jitz] vs [und]\n",
      "Iter= 93000, Average Loss= 9.381128, Average Accuracy= 3.70%\n",
      "['säge,', 'sobald', 'er'] - [nid] vs [der]\n",
      "Iter= 94000, Average Loss= 9.698337, Average Accuracy= 2.50%\n",
      "['Kamin', 'steit', 'e'] - [Pärson,] vs [der]\n",
      "Iter= 95000, Average Loss= 9.708556, Average Accuracy= 2.20%\n",
      "['use', 'zieh.', '«Pärsee,'] - [das] vs [und]\n",
      "Iter= 96000, Average Loss= 9.467513, Average Accuracy= 4.00%\n",
      "['totestill,', 'so', 'daß'] - [me] vs [der]\n",
      "Iter= 97000, Average Loss= 9.484815, Average Accuracy= 5.10%\n",
      "['der', 'Liebegger,', '«das'] - [isch] vs [und]\n",
      "Iter= 98000, Average Loss= 9.415125, Average Accuracy= 3.70%\n",
      "['prächtigs', 'Bild', 'gsi,'] - [dä] vs [der]\n",
      "Iter= 99000, Average Loss= 9.431576, Average Accuracy= 3.70%\n",
      "['Glidere', 'hie', 'bisch,'] - [nid] vs [und]\n",
      "Iter= 100000, Average Loss= 9.414103, Average Accuracy= 3.70%\n",
      "['dasmal', 'mit', 'dezidiertem'] - [Ton.] vs [der]\n",
      "Optimization Finished!\n",
      "Elapsed time:  4.997484661738078 hr\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "\n",
    "writer = tf.summary.FileWriter(config.output_dir)\n",
    "\n",
    "training_data = read_data(config.training_file)\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "model = LSTMModel(config, vocab_size)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "    \n",
    "    train(session, model, config, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(session, config, dictionary, reverse_dictionary):\n",
    "    length_of_sentence_to_produce = 10\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    pred = graph.get_tensor_by_name(\"rnn_predictor:0\")\n",
    "    x = graph.get_tensor_by_name(\"x:0\") \n",
    "    \n",
    "    try_again = \"y\"\n",
    "    \n",
    "    while try_again == \"y\":\n",
    "        prompt = \"%s words: \" % config.n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != config.n_input:\n",
    "            print(\"Wrong number of words\")\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(length_of_sentence_to_produce):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "                print(sentence)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        try_again = input(\"Type 'y' to try again \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "training_data = read_data(config.training_file)\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "\n",
    "new_graph = tf.Graph() # see https://github.com/tensorflow/tensorflow/issues/4603\n",
    "with tf.Session(graph=new_graph) as session:\n",
    "    saver = tf.train.import_meta_graph(config.model_file + '.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint(config.output_dir))\n",
    "    \n",
    "    test(session, config, dictionary, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
