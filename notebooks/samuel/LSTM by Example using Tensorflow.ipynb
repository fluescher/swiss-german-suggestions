{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM by Example using Tensorflow\n",
    "https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return LSTMConfig\n",
    "\n",
    "class LSTMConfig:\n",
    "    # Input / output\n",
    "    training_file = 'LSTM_by_Example_data/belling_the_cat.txt'\n",
    "    output_dir = 'LSTM_by_Example_output'\n",
    "    model_file = output_dir + '/LSTM_by_Example_model'\n",
    "    \n",
    "    # Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 50000\n",
    "    display_step = 1000\n",
    "    n_input = 3\n",
    "\n",
    "    # number of units in RNN cell\n",
    "    n_hidden = 512\n",
    "    \n",
    "    # Use two layer RNN cells\n",
    "    two_layer = False\n",
    "    \n",
    "    predictor_name = \"rnn_predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(config, x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, config.n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x, config.n_input, 1)\n",
    "\n",
    "    if config.two_layer:\n",
    "        # 2-layer LSTM, each layer has n_hidden units.\n",
    "        # Average Accuracy= 95.20% at 50k iter\n",
    "        rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(config.n_hidden), rnn.BasicLSTMCell(config.n_hidden)])\n",
    "    else:\n",
    "        # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "        # Average Accuracy= 90.60% 50k iter\n",
    "        rnn_cell = rnn.BasicLSTMCell(config.n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.add(tf.matmul(outputs[-1], weights['out']), biases['out'], name=config.predictor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "\n",
    "    def __init__(self, config, vocab_size):\n",
    "        # tf Graph input\n",
    "        x = tf.placeholder(\"float\", [None, config.n_input, 1], name='x')\n",
    "        y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "        # RNN output node weights and biases\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([config.n_hidden, vocab_size]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "        }\n",
    "    \n",
    "        pred = RNN(config, x, weights, biases)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # TODO: cleanup\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.pred = pred\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session, model, config, training_data):\n",
    "    step = 0\n",
    "    offset = random.randint(0, config.n_input+1)\n",
    "    end_offset = config.n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    # save meta model and prepare saver for state\n",
    "    tf.train.export_meta_graph(filename=config.model_file)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while step < config.training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, config.n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+config.n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+config.n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([model.optimizer, model.accuracy, model.cost, model.pred], \\\n",
    "                                                feed_dict={model.x: symbols_in_keys, model.y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % config.display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/config.display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/config.display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + config.n_input)]\n",
    "            symbols_out = training_data[offset + config.n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "            # save the current state\n",
    "            saver.save(session, config.model_file,global_step=step+1, write_meta_graph=False)\n",
    "        step += 1\n",
    "        offset += (config.n_input + 1)\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 5.949724, Average Accuracy= 3.60%\n",
      "['nobody', 'spoke', '.'] - [then] vs [cat]\n",
      "Iter= 2000, Average Loss= 3.875439, Average Accuracy= 12.50%\n",
      "['who', 'is', 'to'] - [bell] vs [i]\n",
      "Iter= 3000, Average Loss= 2.931354, Average Accuracy= 29.40%\n",
      "['old', 'mouse', 'got'] - [up] vs [up]\n",
      "Iter= 4000, Average Loss= 2.966577, Average Accuracy= 31.40%\n",
      "['she', 'was', 'in'] - [the] vs [,]\n",
      "Iter= 5000, Average Loss= 2.135666, Average Accuracy= 48.10%\n",
      "['the', 'neck', 'of'] - [the] vs [,]\n",
      "Iter= 6000, Average Loss= 2.061857, Average Accuracy= 47.20%\n",
      "['that', 'a', 'small'] - [bell] vs [bell]\n",
      "Iter= 7000, Average Loss= 1.818979, Average Accuracy= 56.50%\n",
      "['could', 'receive', 'some'] - [signal] vs [signal]\n",
      "Iter= 8000, Average Loss= 1.535870, Average Accuracy= 61.50%\n",
      "['he', ',', 'that'] - [our] vs [our]\n",
      "Iter= 9000, Average Loss= 1.424246, Average Accuracy= 67.10%\n",
      "['last', 'a', 'young'] - [mouse] vs [their]\n",
      "Iter= 10000, Average Loss= 1.475155, Average Accuracy= 65.50%\n",
      "['outwit', 'their', 'common'] - [enemy] vs [she]\n",
      "Iter= 11000, Average Loss= 1.279215, Average Accuracy= 68.00%\n",
      "['to', 'consider', 'what'] - [measures] vs [measures]\n",
      "Iter= 12000, Average Loss= 1.231071, Average Accuracy= 70.20%\n",
      "['old', 'mouse', 'said'] - [it] vs [it]\n",
      "Iter= 13000, Average Loss= 1.148723, Average Accuracy= 71.20%\n",
      "['?', 'the', 'mice'] - [looked] vs [looked]\n",
      "Iter= 14000, Average Loss= 1.100469, Average Accuracy= 72.70%\n",
      "['mouse', 'got', 'up'] - [and] vs [and]\n",
      "Iter= 15000, Average Loss= 1.012346, Average Accuracy= 76.40%\n",
      "[',', 'until', 'an'] - [old] vs [old]\n",
      "Iter= 16000, Average Loss= 1.102009, Average Accuracy= 74.80%\n",
      "[',', 'and', 'could'] - [easily] vs [easily]\n",
      "Iter= 17000, Average Loss= 0.853914, Average Accuracy= 81.20%\n",
      "['bell', 'be', 'procured'] - [,] vs [her]\n",
      "Iter= 18000, Average Loss= 0.871027, Average Accuracy= 78.10%\n",
      "[',', 'therefore', ','] - [to] vs [to]\n",
      "Iter= 19000, Average Loss= 0.736251, Average Accuracy= 80.90%\n",
      "['of', 'her', 'approach'] - [,] vs [,]\n",
      "Iter= 20000, Average Loss= 0.769267, Average Accuracy= 81.60%\n",
      "['treacherous', 'manner', 'in'] - [which] vs [was]\n",
      "Iter= 21000, Average Loss= 0.700577, Average Accuracy= 84.30%\n",
      "['all', 'agree', ','] - [said] vs [said]\n",
      "Iter= 22000, Average Loss= 0.654995, Average Accuracy= 85.90%\n",
      "['said', 'he', 'had'] - [a] vs [a]\n",
      "Iter= 23000, Average Loss= 0.739791, Average Accuracy= 83.50%\n",
      "['some', 'said', 'this'] - [,] vs [,]\n",
      "Iter= 24000, Average Loss= 0.718751, Average Accuracy= 82.00%\n",
      "['outwit', 'their', 'common'] - [enemy] vs [enemy]\n",
      "Iter= 25000, Average Loss= 0.666324, Average Accuracy= 84.40%\n",
      "['it', 'is', 'easy'] - [to] vs [to]\n",
      "Iter= 26000, Average Loss= 0.682614, Average Accuracy= 85.30%\n",
      "['very', 'well', ','] - [but] vs [but]\n",
      "Iter= 27000, Average Loss= 0.622087, Average Accuracy= 84.50%\n",
      "['met', 'with', 'general'] - [applause] vs [,]\n",
      "Iter= 28000, Average Loss= 0.698455, Average Accuracy= 84.90%\n",
      "['while', 'she', 'was'] - [in] vs [in]\n",
      "Iter= 29000, Average Loss= 0.582060, Average Accuracy= 85.30%\n",
      "[',', 'and', 'could'] - [easily] vs [easily]\n",
      "Iter= 30000, Average Loss= 0.645160, Average Accuracy= 85.10%\n",
      "['attached', 'by', 'a'] - [ribbon] vs [ribbon]\n",
      "Iter= 31000, Average Loss= 0.589152, Average Accuracy= 85.70%\n",
      "['venture', ',', 'therefore'] - [,] vs [,]\n",
      "Iter= 32000, Average Loss= 0.556159, Average Accuracy= 86.20%\n",
      "['could', 'easily', 'escape'] - [from] vs [while]\n",
      "Iter= 33000, Average Loss= 0.591977, Average Accuracy= 84.60%\n",
      "['manner', 'in', 'which'] - [the] vs [the]\n",
      "Iter= 34000, Average Loss= 0.514075, Average Accuracy= 87.30%\n",
      "['case', '.', 'you'] - [will] vs [will]\n",
      "Iter= 35000, Average Loss= 0.632792, Average Accuracy= 86.20%\n",
      "['make', ',', 'which'] - [he] vs [he]\n",
      "Iter= 36000, Average Loss= 0.424861, Average Accuracy= 89.40%\n",
      "['at', 'last', 'a'] - [young] vs [young]\n",
      "Iter= 37000, Average Loss= 0.474506, Average Accuracy= 89.40%\n",
      "['cat', '.', 'some'] - [said] vs [said]\n",
      "Iter= 38000, Average Loss= 0.427339, Average Accuracy= 89.50%\n",
      "['measures', 'they', 'could'] - [take] vs [take]\n",
      "Iter= 39000, Average Loss= 0.394916, Average Accuracy= 90.00%\n",
      "['it', 'is', 'easy'] - [to] vs [to]\n",
      "Iter= 40000, Average Loss= 0.548041, Average Accuracy= 87.50%\n",
      "['?', 'the', 'mice'] - [looked] vs [looked]\n",
      "Iter= 41000, Average Loss= 0.488449, Average Accuracy= 87.90%\n",
      "['said', 'that', 'is'] - [all] vs [all]\n",
      "Iter= 42000, Average Loss= 0.411330, Average Accuracy= 89.10%\n",
      "['proposal', 'met', 'with'] - [general] vs [general]\n",
      "Iter= 43000, Average Loss= 0.320818, Average Accuracy= 91.00%\n",
      "['when', 'she', 'was'] - [about] vs [about]\n",
      "Iter= 44000, Average Loss= 0.426520, Average Accuracy= 89.20%\n",
      "['by', 'a', 'ribbon'] - [round] vs [round]\n",
      "Iter= 45000, Average Loss= 0.344393, Average Accuracy= 91.00%\n",
      "['escape', 'from', 'her'] - [.] vs [agree]\n",
      "Iter= 46000, Average Loss= 0.384807, Average Accuracy= 90.60%\n",
      "['approach', ',', 'we'] - [could] vs [could]\n",
      "Iter= 47000, Average Loss= 0.397071, Average Accuracy= 89.50%\n",
      "['if', 'we', 'could'] - [receive] vs [receive]\n",
      "Iter= 48000, Average Loss= 0.313956, Average Accuracy= 91.60%\n",
      "['treacherous', 'manner', 'in'] - [which] vs [which]\n",
      "Iter= 49000, Average Loss= 0.255310, Average Accuracy= 92.50%\n",
      "['thought', 'would', 'meet'] - [the] vs [the]\n",
      "Iter= 50000, Average Loss= 0.327303, Average Accuracy= 92.10%\n",
      "['said', 'that', 'but'] - [at] vs [at]\n",
      "Optimization Finished!\n",
      "Elapsed time:  7.605316627025604 min\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "\n",
    "#writer = tf.summary.FileWriter(config.output_dir)\n",
    "\n",
    "training_data = read_data(config.training_file)\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "model = LSTMModel(config, vocab_size)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    #writer.add_graph(session.graph)\n",
    "    \n",
    "    train(session, model, config, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(session, config, dictionary, reverse_dictionary):\n",
    "    length_of_sentence_to_produce = 10\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    pred = graph.get_tensor_by_name(\"rnn_predictor:0\")\n",
    "    x = graph.get_tensor_by_name(\"x:0\") \n",
    "    \n",
    "    try_again = \"y\"\n",
    "    \n",
    "    while try_again == \"y\":\n",
    "        prompt = \"%s words: \" % config.n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != config.n_input:\n",
    "            print(\"Wrong number of words\")\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(length_of_sentence_to_produce):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "                print(sentence)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        try_again = input(\"Type 'y' to try again \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from LSTM_by_Example_output/LSTM_by_Example_model-50000\n",
      "3 words: the mice would\n",
      "the mice would us\n",
      "the mice would us ,\n",
      "the mice would us , the\n",
      "the mice would us , the propose\n",
      "the mice would us , the propose had\n",
      "the mice would us , the propose had a\n",
      "the mice would us , the propose had a said\n",
      "the mice would us , the propose had a said that\n",
      "the mice would us , the propose had a said that all\n",
      "the mice would us , the propose had a said that all a\n",
      "Type 'y' to try again y\n",
      "3 words: the mice said\n",
      "the mice said said\n",
      "the mice said said that\n",
      "the mice said said that all\n",
      "the mice said said that all a\n",
      "the mice said said that all a make\n",
      "the mice said said that all a make round\n",
      "the mice said said that all a make round know\n",
      "the mice said said that all a make round know she\n",
      "the mice said said that all a make round know she was\n",
      "the mice said said that all a make round know she was about\n",
      "Type 'y' to try again y\n",
      "3 words: the council should\n",
      "the council should all\n",
      "the council should all in\n",
      "the council should all in the\n",
      "the council should all in the neighbourhood\n",
      "the council should all in the neighbourhood .\n",
      "the council should all in the neighbourhood . this\n",
      "the council should all in the neighbourhood . this proposal\n",
      "the council should all in the neighbourhood . this proposal met\n",
      "the council should all in the neighbourhood . this proposal met with\n",
      "the council should all in the neighbourhood . this proposal met with general\n",
      "Type 'y' to try again y\n",
      "3 words: said the mice\n",
      "said the mice bell\n",
      "said the mice bell a\n",
      "said the mice bell a general\n",
      "said the mice bell a general to\n",
      "said the mice bell a general to make\n",
      "said the mice bell a general to make ,\n",
      "said the mice bell a general to make , which\n",
      "said the mice bell a general to make , which he\n",
      "said the mice bell a general to make , which he thought\n",
      "said the mice bell a general to make , which he thought would\n",
      "Type 'y' to try again \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "training_data = read_data(config.training_file)\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "\n",
    "new_graph = tf.Graph() # see https://github.com/tensorflow/tensorflow/issues/4603\n",
    "with tf.Session(graph=new_graph) as session:\n",
    "    saver = tf.train.import_meta_graph(config.model_file)\n",
    "    saver.restore(session, tf.train.latest_checkpoint(config.output_dir))\n",
    "    \n",
    "    test(session, config, dictionary, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
