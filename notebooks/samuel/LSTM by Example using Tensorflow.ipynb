{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM by Example using Tensorflow\n",
    "https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return LSTMConfig\n",
    "\n",
    "class LSTMConfig:\n",
    "    # Input / output\n",
    "    training_file = 'LSTM_by_Example_data/belling_the_cat.txt'\n",
    "    logs_path = 'LSTM_by_Example_output'\n",
    "    \n",
    "    # Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 50000\n",
    "    display_step = 1000\n",
    "    n_input = 3\n",
    "\n",
    "    # number of units in RNN cell\n",
    "    n_hidden = 512\n",
    "    \n",
    "    # Use two layer RNN celss\n",
    "    two_layer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(config, x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, config.n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x, config.n_input, 1)\n",
    "\n",
    "    if config.two_layer:\n",
    "        # 2-layer LSTM, each layer has n_hidden units.\n",
    "        # Average Accuracy= 95.20% at 50k iter\n",
    "        rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(config.n_hidden), rnn.BasicLSTMCell(config.n_hidden)])\n",
    "    else:\n",
    "        # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "        # Average Accuracy= 90.60% 50k iter\n",
    "        rnn_cell = rnn.BasicLSTMCell(config.n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "\n",
    "    def __init__(self, config, vocab_size):\n",
    "        # tf Graph input\n",
    "        x = tf.placeholder(\"float\", [None, config.n_input, 1])\n",
    "        y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "        # RNN output node weights and biases\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([config.n_hidden, vocab_size]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "        }\n",
    "    \n",
    "        pred = RNN(config, x, weights, biases)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # TODO: cleanup\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.pred = pred\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session, model, config, training_data):\n",
    "    step = 0\n",
    "    offset = random.randint(0, config.n_input+1)\n",
    "    end_offset = config.n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while step < config.training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, config.n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+config.n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+config.n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([model.optimizer, model.accuracy, model.cost, model.pred], \\\n",
    "                                                feed_dict={model.x: symbols_in_keys, model.y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % config.display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/config.display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/config.display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + config.n_input)]\n",
    "            symbols_out = training_data[offset + config.n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (config.n_input + 1)\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 5.261949, Average Accuracy= 6.50%\n",
      "['cat', '?', 'the'] - [mice] vs [therefore]\n",
      "Iter= 2000, Average Loss= 3.749449, Average Accuracy= 16.00%\n",
      "['proposal', 'met', 'with'] - [general] vs [,]\n",
      "Iter= 3000, Average Loss= 3.647610, Average Accuracy= 20.60%\n",
      "['we', 'should', 'always'] - [know] vs [in]\n",
      "Iter= 4000, Average Loss= 2.735648, Average Accuracy= 35.70%\n",
      "['by', 'a', 'ribbon'] - [round] vs [some]\n",
      "Iter= 5000, Average Loss= 2.418080, Average Accuracy= 43.70%\n",
      "[',', 'therefore', ','] - [to] vs [if]\n",
      "Iter= 6000, Average Loss= 2.204940, Average Accuracy= 46.50%\n",
      "['her', 'approach', ','] - [we] vs [we]\n",
      "Iter= 7000, Average Loss= 1.962611, Average Accuracy= 52.20%\n",
      "['we', 'could', 'easily'] - [escape] vs [escape]\n",
      "Iter= 8000, Average Loss= 1.642149, Average Accuracy= 60.30%\n",
      "['we', 'could', 'receive'] - [some] vs [round]\n",
      "Iter= 9000, Average Loss= 1.350225, Average Accuracy= 70.40%\n",
      "['manner', 'in', 'which'] - [the] vs [the]\n",
      "Iter= 10000, Average Loss= 1.332675, Average Accuracy= 69.60%\n",
      "['he', ',', 'that'] - [our] vs [our]\n",
      "Iter= 11000, Average Loss= 1.247299, Average Accuracy= 70.70%\n",
      "['proposal', 'to', 'make'] - [,] vs [,]\n",
      "Iter= 12000, Average Loss= 1.085715, Average Accuracy= 75.40%\n",
      "['he', 'had', 'a'] - [proposal] vs [proposal]\n",
      "Iter= 13000, Average Loss= 1.279718, Average Accuracy= 69.00%\n",
      "['and', 'said', 'he'] - [had] vs [is]\n",
      "Iter= 14000, Average Loss= 1.016622, Average Accuracy= 76.70%\n",
      "['outwit', 'their', 'common'] - [enemy] vs [enemy]\n",
      "Iter= 15000, Average Loss= 1.188918, Average Accuracy= 74.00%\n",
      "['enemy', ',', 'the'] - [cat] vs [cat]\n",
      "Iter= 16000, Average Loss= 1.044405, Average Accuracy= 76.70%\n",
      "['to', 'outwit', 'their'] - [common] vs [common]\n",
      "Iter= 17000, Average Loss= 0.769488, Average Accuracy= 81.50%\n",
      "['.', 'then', 'the'] - [old] vs [if]\n",
      "Iter= 18000, Average Loss= 0.909729, Average Accuracy= 78.40%\n",
      "['is', 'all', 'very'] - [well] vs [well]\n",
      "Iter= 19000, Average Loss= 0.875315, Average Accuracy= 78.60%\n",
      "['old', 'mouse', 'got'] - [up] vs [up]\n",
      "Iter= 20000, Average Loss= 0.653613, Average Accuracy= 84.80%\n",
      "['the', 'neighbourhood', '.'] - [this] vs [this]\n",
      "Iter= 21000, Average Loss= 0.617081, Average Accuracy= 86.40%\n",
      "['while', 'she', 'was'] - [in] vs [in]\n",
      "Iter= 22000, Average Loss= 0.833737, Average Accuracy= 80.90%\n",
      "['the', 'cat', '.'] - [by] vs [some]\n",
      "Iter= 23000, Average Loss= 0.602768, Average Accuracy= 85.80%\n",
      "['to', 'propose', 'that'] - [a] vs [a]\n",
      "Iter= 24000, Average Loss= 0.711094, Average Accuracy= 83.10%\n",
      "['now', ',', 'if'] - [we] vs [we]\n",
      "Iter= 25000, Average Loss= 0.409913, Average Accuracy= 90.00%\n",
      "['said', 'he', ','] - [that] vs [that]\n",
      "Iter= 26000, Average Loss= 0.629709, Average Accuracy= 85.40%\n",
      "['thought', 'would', 'meet'] - [the] vs [the]\n",
      "Iter= 27000, Average Loss= 0.624619, Average Accuracy= 86.20%\n",
      "['got', 'up', 'and'] - [said] vs [said]\n",
      "Iter= 28000, Average Loss= 0.572142, Average Accuracy= 86.00%\n",
      "['this', ',', 'and'] - [some] vs [some]\n",
      "Iter= 29000, Average Loss= 0.716566, Average Accuracy= 84.60%\n",
      "['the', 'mice', 'had'] - [a] vs [a]\n",
      "Iter= 30000, Average Loss= 0.525360, Average Accuracy= 87.50%\n",
      "['at', 'one', 'another'] - [and] vs [and]\n",
      "Iter= 31000, Average Loss= 0.499595, Average Accuracy= 88.20%\n",
      "['mouse', 'got', 'up'] - [and] vs [and]\n",
      "Iter= 32000, Average Loss= 0.382890, Average Accuracy= 91.40%\n",
      "['retire', 'while', 'she'] - [was] vs [was]\n",
      "Iter= 33000, Average Loss= 0.642990, Average Accuracy= 85.70%\n",
      "['was', 'about', ','] - [and] vs [and]\n",
      "Iter= 34000, Average Loss= 0.492212, Average Accuracy= 88.70%\n",
      "['.', 'by', 'this'] - [means] vs [means]\n",
      "Iter= 35000, Average Loss= 0.448851, Average Accuracy= 89.10%\n",
      "[',', 'and', 'attached'] - [by] vs [by]\n",
      "Iter= 36000, Average Loss= 0.394728, Average Accuracy= 90.40%\n",
      "['of', 'her', 'approach'] - [,] vs [,]\n",
      "Iter= 37000, Average Loss= 0.451844, Average Accuracy= 88.90%\n",
      "['manner', 'in', 'which'] - [the] vs [the]\n",
      "Iter= 38000, Average Loss= 0.418850, Average Accuracy= 90.40%\n",
      "['all', 'agree', ','] - [said] vs [said]\n",
      "Iter= 39000, Average Loss= 0.414003, Average Accuracy= 89.20%\n",
      "['that', 'but', 'at'] - [last] vs [last]\n",
      "Iter= 40000, Average Loss= 0.375719, Average Accuracy= 91.70%\n",
      "['outwit', 'their', 'common'] - [enemy] vs [enemy]\n",
      "Iter= 41000, Average Loss= 0.331826, Average Accuracy= 91.60%\n",
      "['the', 'mice', 'had'] - [a] vs [a]\n",
      "Iter= 42000, Average Loss= 0.417450, Average Accuracy= 90.10%\n",
      "['the', 'mice', 'looked'] - [at] vs [at]\n",
      "Iter= 43000, Average Loss= 0.516138, Average Accuracy= 88.80%\n",
      "['well', ',', 'but'] - [who] vs [who]\n",
      "Iter= 44000, Average Loss= 0.403679, Average Accuracy= 90.80%\n",
      "['with', 'general', 'applause'] - [,] vs [always]\n",
      "Iter= 45000, Average Loss= 0.437003, Average Accuracy= 89.40%\n",
      "['this', 'means', 'we'] - [should] vs [should]\n",
      "Iter= 46000, Average Loss= 0.395477, Average Accuracy= 90.90%\n",
      "['her', '.', 'i'] - [venture] vs [venture]\n",
      "Iter= 47000, Average Loss= 0.323879, Average Accuracy= 92.40%\n",
      "['we', 'could', 'easily'] - [escape] vs [escape]\n",
      "Iter= 48000, Average Loss= 0.485511, Average Accuracy= 89.20%\n",
      "['approaches', 'us', '.'] - [now] vs [now]\n",
      "Iter= 49000, Average Loss= 0.448288, Average Accuracy= 89.80%\n",
      "['he', ',', 'that'] - [our] vs [our]\n",
      "Iter= 50000, Average Loss= 0.437828, Average Accuracy= 90.10%\n",
      "['he', 'thought', 'would'] - [meet] vs [meet]\n",
      "Optimization Finished!\n",
      "Elapsed time:  7.980486488342285 min\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "\n",
    "writer = tf.summary.FileWriter(config.logs_path)\n",
    "\n",
    "training_data = read_data(config.training_file)\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "model = LSTMModel(config, vocab_size)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "    \n",
    "    train(session, model, config, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(session, model, config, dictionary, reverse_dictionary):\n",
    "    prompt = \"%s words: \" % config.n_input\n",
    "    sentence = input(prompt)\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != config.n_input:\n",
    "        print(\"Wrong number of words\")\n",
    "    try:\n",
    "        symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "        for i in range(32):\n",
    "            keys = np.reshape(np.array(symbols_in_keys), [-1, config.n_input, 1])\n",
    "            onehot_pred = session.run(model.pred, feed_dict={model.x: keys})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 words: the mice would\n",
      "the mice would he\n",
      "the mice would he he\n",
      "the mice would he he he\n",
      "the mice would he he he he\n",
      "the mice would he he he he he\n",
      "the mice would he he he he he he\n",
      "the mice would he he he he he he he\n",
      "the mice would he he he he he he he he\n",
      "the mice would he he he he he he he he he\n",
      "the mice would he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "the mice would he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    test(session, model, config, dictionary, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
