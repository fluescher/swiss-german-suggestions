{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it\n",
    "\n",
    "To run it use\n",
    "\n",
    "`docker build -f Keras.Dockerfile -t keras .`\n",
    "\n",
    "`docker run -v ${PWD}:/notebooks -p 8888:8888 keras` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_sentences(file_path):\n",
    "    from xml.etree import cElementTree as ET\n",
    "    t = ET.parse(file_path)\n",
    "    sentences = []\n",
    "    for article in list(t.getroot()):\n",
    "        for sentence in  list(article.findall(\"s\")):\n",
    "            cur = \"\"\n",
    "            for word in list(sentence):\n",
    "                if word.text is None:\n",
    "                    continue\n",
    "                    \n",
    "                cur = cur + \" \" + word.text\n",
    "                \n",
    "            sentences.append(cur)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blick = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/blick.xml')\n",
    "blogs = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/blogs.xml')\n",
    "schobinger = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/schobinger.xml')\n",
    "swatch = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/swatch.xml')\n",
    "wiki = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/wiki.xml')\n",
    "\n",
    "all_sentences = blick + blogs + schobinger + swatch + wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7455\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(seqs):\n",
    "    seq = []\n",
    "    for s in seqs:\n",
    "        seq = seq + s\n",
    "    return seq\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def window(seq, n=2):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_words = 3000\n",
    "max_length = 3\n",
    "\n",
    "t = Tokenizer(num_words=num_words,\n",
    "           filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’\\'',\n",
    "           lower=True,\n",
    "           split=\" \",\n",
    "           char_level=False)\n",
    "t.fit_on_texts(all_sentences)\n",
    "\n",
    "def lookup_word(index):\n",
    "    for e in t.word_index:\n",
    "        if t.word_index[e] == index:\n",
    "            return e\n",
    "        \n",
    "    return \"<unk>\"\n",
    "\n",
    "seqs = t.texts_to_sequences(all_sentences)\n",
    "seq = flatten(seqs)\n",
    "\n",
    "data = list(window(seq, max_length+1))\n",
    "\n",
    "xs = [x[:max_length] for x in data]\n",
    "ys = to_categorical([x[max_length] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 128)            384000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3000)              387000    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3000)              0         \n",
      "=================================================================\n",
      "Total params: 902,584\n",
      "Trainable params: 902,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 128, input_length=max_length))\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dense(num_words))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "74792/74792 [==============================] - 64s 850us/step - loss: 6.6689 - acc: 0.0378\n",
      "Epoch 2/30\n",
      "74792/74792 [==============================] - 62s 827us/step - loss: 6.2942 - acc: 0.0546\n",
      "Epoch 3/30\n",
      "74792/74792 [==============================] - 63s 839us/step - loss: 5.9609 - acc: 0.0732\n",
      "Epoch 4/30\n",
      "74792/74792 [==============================] - 62s 831us/step - loss: 5.6998 - acc: 0.0856\n",
      "Epoch 5/30\n",
      "74792/74792 [==============================] - 64s 850us/step - loss: 5.4559 - acc: 0.0975\n",
      "Epoch 6/30\n",
      "74792/74792 [==============================] - 63s 840us/step - loss: 5.2112 - acc: 0.1094\n",
      "Epoch 7/30\n",
      "74792/74792 [==============================] - 64s 856us/step - loss: 4.9512 - acc: 0.1227\n",
      "Epoch 8/30\n",
      "74792/74792 [==============================] - 64s 856us/step - loss: 4.6718 - acc: 0.1367\n",
      "Epoch 9/30\n",
      "74792/74792 [==============================] - 63s 847us/step - loss: 4.3840 - acc: 0.1537\n",
      "Epoch 10/30\n",
      "74792/74792 [==============================] - 63s 841us/step - loss: 4.0930 - acc: 0.1792\n",
      "Epoch 11/30\n",
      "74792/74792 [==============================] - 63s 839us/step - loss: 3.8166 - acc: 0.2084\n",
      "Epoch 12/30\n",
      "74792/74792 [==============================] - 63s 845us/step - loss: 3.5614 - acc: 0.2430\n",
      "Epoch 13/30\n",
      "74792/74792 [==============================] - 63s 840us/step - loss: 3.3285 - acc: 0.2802\n",
      "Epoch 14/30\n",
      "74792/74792 [==============================] - 64s 851us/step - loss: 3.1183 - acc: 0.3152\n",
      "Epoch 15/30\n",
      "74792/74792 [==============================] - 63s 844us/step - loss: 2.9305 - acc: 0.3493\n",
      "Epoch 16/30\n",
      "74792/74792 [==============================] - 64s 860us/step - loss: 2.7592 - acc: 0.3797\n",
      "Epoch 17/30\n",
      "74792/74792 [==============================] - 65s 863us/step - loss: 2.5982 - acc: 0.4128\n",
      "Epoch 18/30\n",
      "74792/74792 [==============================] - 63s 841us/step - loss: 2.4634 - acc: 0.4358\n",
      "Epoch 19/30\n",
      "74792/74792 [==============================] - 63s 846us/step - loss: 2.3280 - acc: 0.4646\n",
      "Epoch 20/30\n",
      "74792/74792 [==============================] - 65s 863us/step - loss: 2.2085 - acc: 0.4849\n",
      "Epoch 21/30\n",
      "74792/74792 [==============================] - 68s 903us/step - loss: 2.1047 - acc: 0.5072\n",
      "Epoch 22/30\n",
      "74792/74792 [==============================] - 68s 916us/step - loss: 2.0096 - acc: 0.5257\n",
      "Epoch 23/30\n",
      "74792/74792 [==============================] - 65s 866us/step - loss: 1.9207 - acc: 0.5431\n",
      "Epoch 24/30\n",
      "74792/74792 [==============================] - 72s 957us/step - loss: 1.8339 - acc: 0.5630\n",
      "Epoch 25/30\n",
      "74792/74792 [==============================] - 66s 877us/step - loss: 1.7561 - acc: 0.5800\n",
      "Epoch 26/30\n",
      "74792/74792 [==============================] - 66s 880us/step - loss: 1.6893 - acc: 0.5933\n",
      "Epoch 27/30\n",
      "74792/74792 [==============================] - 66s 877us/step - loss: 1.6223 - acc: 0.6080\n",
      "Epoch 28/30\n",
      "74792/74792 [==============================] - 65s 870us/step - loss: 1.5636 - acc: 0.6227\n",
      "Epoch 29/30\n",
      "74792/74792 [==============================] - 66s 885us/step - loss: 1.5042 - acc: 0.6347\n",
      "Epoch 30/30\n",
      "74792/74792 [==============================] - 66s 879us/step - loss: 1.4579 - acc: 0.6428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcadb484828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(xs), np.array(ys), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ander\n",
      "emal\n",
      "wììrt\n",
      "soo\n",
      "rächt\n",
      "chönt\n",
      "so\n",
      "chöne\n",
      "uf\n",
      "«\n"
     ]
    }
   ],
   "source": [
    "input = t.texts_to_sequences([\"ich zmitzt im\"])\n",
    "result = model.predict(np.array(input))\n",
    "\n",
    "top = sorted(range(len(result[0])), key=lambda i: result[0][i], reverse=True)[:10]\n",
    "for i in top:\n",
    "    print(lookup_word(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ich', 'bin', 'so', 'froh', 'dass', 'ich', 'mis', 'lebe', 'do', 'id', 'stadt', 'go', 'luege', 'und', 'ned', 'z', 'herzig', 'gsi', 'und', 'sie', 'sind', 'soooo', 'fein', 'das', 'hets', 'öpe', 'am', 'bahnhof', 'la', 'inere', 'uf', 'e', 'insle', 'd', 'sunne', 'im', 'herze', 'für', 'franke', 'das', 'isch', 'echt', 'cool', 'gsi', 'usserdem', 'hets', 'echt', 'gha', 'womer', 'wo', 'zäme', 'sehr', 'viel', 'da', 'werded', 'afange', 'hani', 'grad', 'na', 'muese', 'etc', 'etc', 'uf', 'jede', 'fall', 'anen', 'aalass', 'ich', 'sogar', 'e', 'richtigi', 'oben', 'isches', 'so', 'angscht', 'für', 'en', 'wo', 'ab', 'recht', 'em', 'da', 'und', 'uf', 'de', 'foeteli', 'gsehnd', 'ish', 'd', 'au', 'vom', 'und', 'dä', 'vom', 'unternehmä', 'hayek', 'sowiä', 'dank', 'dä', 'und', 'dä', 'vom', 'unternehmä']\n"
     ]
    }
   ],
   "source": [
    "def generate(a, b, c, accl=[], pos=0, max=10):\n",
    "    if pos == 0:\n",
    "        accl = [a,b,c]\n",
    "    if pos == max:\n",
    "        print(accl)\n",
    "        return\n",
    "    \n",
    "    input = t.texts_to_sequences([a + \" \" + b + \" \" + c])\n",
    "    result = model.predict(np.array(input)) \n",
    "    \n",
    "    top = sorted(range(len(result[0])), key=lambda i: result[0][i], reverse=True)[:10]\n",
    "    w = lookup_word(top[0])\n",
    "    \n",
    "    accl.append(w)\n",
    "    generate(b, c, w, accl, pos+1, max)\n",
    "    \n",
    "generate(\"ich\", \"bin\", \"so\", max=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
