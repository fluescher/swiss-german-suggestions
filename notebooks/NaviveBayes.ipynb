{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Naive Bayes Model with NLTK to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_sentences(file_path):\n",
    "    from xml.etree import cElementTree as ET\n",
    "    t = ET.parse(file_path)\n",
    "    sentences = []\n",
    "    for article in list(t.getroot()):\n",
    "        \n",
    "        for sentence in list(article):\n",
    "            cur = []\n",
    "            for word in list(sentence):\n",
    "                cur.append(word.text)\n",
    "                \n",
    "            sentences.append(cur)\n",
    "        \n",
    "    return sentences\n",
    "        \n",
    "    \n",
    "blick = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/blick.xml')\n",
    "blogs = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/blogs.xml')\n",
    "schobinger = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/schobinger.xml')\n",
    "swatch = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/swatch.xml')\n",
    "wiki = load_sentences('../data/NOAHsCorpusOfSwissGermanDialects_Release2.1/wiki.xml')\n",
    "\n",
    "\n",
    "\n",
    "#all_sentences = blick + blogs + schobinger + swatch + wiki\n",
    "all_sentences = blick + blogs\n",
    "\n",
    "shuffle(all_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build feature from a word and feauterset (tuple with feautre and label) from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(word):\n",
    "    \n",
    "    f = {}\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        f['single'] = word\n",
    "    \n",
    "    for n in list(ngrams(word, 2)):\n",
    "        f[n] = True\n",
    "        \n",
    "    return f\n",
    "\n",
    "\n",
    "def build_featureset(sentences):\n",
    "    featureset = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "    \n",
    "        for i, token in enumerate(sentence):\n",
    "            if i < len(sentence) - 1 and token is not None and len(token) > 1:\n",
    "                # tuple -> (feature, label)\n",
    "                featureset.append((build_feature(token), sentence[i + 1]))\n",
    "    \n",
    "    return featureset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in test and traing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 37918\n",
      "test_set: 500\n"
     ]
    }
   ],
   "source": [
    "featuresets = build_featureset(all_sentences)\n",
    "\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "print('train_set:', len(train_set))\n",
    "print('test_set:', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  7966\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('labels: ', len(classifier.labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best metric for our multi label problem, but for our example we stick with 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.026\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random sentences from the naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buslinie äscht gschmolze zerstört würke seriös Mord entwicklet Mirasol-technik flopped ersetze\n",
      "---\n",
      "Frankriich prüglet überstoht khai tuctucsit usanand setzand unterstützt ort Lauderdale vorhande\n",
      "---\n",
      "isch echt cool ghänged usezögere Mischig Vorfahrä damiti Profässer Von ehre\n",
      "---\n",
      "d .\n",
      "---\n",
      "Woatan besichtige Gc-fän chöi säch verbruuched vorzgah svizzera wärded übersetzt wirde\n",
      "---\n",
      "Sternbilder verhänkt hetmich zuechund gmachd Brüst zämekratze undernoh Regio ungerteilt games\n",
      "---\n",
      "Herkunft hönd wehre Frönd wehre Frönd wehre Frönd wehre Frönd wehre\n",
      "---\n",
      "ryychi zerschtört wirsch „ .\n",
      "---\n",
      "Faltejura Lift ufefahre damiti Profässer Von ehre Frönd wehre Frönd wehre\n",
      "---\n",
      "; .\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "flattened = [val for sublist in wiki for val in sublist]\n",
    "\n",
    "def get_random(e):\n",
    "    \n",
    "    return random.choice(e)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    token = get_random(flattened)\n",
    "\n",
    "    max_tokens = 10\n",
    "    count = 0\n",
    "    \n",
    "    sentence = []\n",
    "    \n",
    "    while(True):\n",
    "        sentence.append(token)\n",
    "        \n",
    "        if token in ['.', '?', '!']:\n",
    "            break\n",
    "\n",
    "        if count >= max_tokens:\n",
    "            break\n",
    "\n",
    "\n",
    "        token = classifier.classify(build_feature(token))\n",
    "        count += 1\n",
    "\n",
    "    print(' '.join(sentence))\n",
    "    print('---')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
